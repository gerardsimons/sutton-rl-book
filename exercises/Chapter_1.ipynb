{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exercise 1.1: Self-Play \n",
    "\n",
    "*Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In such a case I would expect it to be much more difficult for the agent to win. Meaning it would have to come up with more complex moves to beat the other agent (itself). Although a random player would in the long run play every possible move as well (the law of large numbers), most of its moves would not be very effective. If an RL agent would play against itself, it would come up with counter moves for its own moves, something a random agent would not do. This would make it learn more complex policies much more rapidly until eventually exhausting every kind of strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exercise 1.2: Symmetries\n",
    "\n",
    "_ Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Mapping symmetries to a single game state will greatly reduce the game state size and make training easier as any symmetrical game state would be mapped to a previously played game state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exercise 1.3: Greedy Play\n",
    "    \n",
    "_Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In such a simple game I would expect it to do reasonably well. Nevertheless there might be occurrences of short-sightedness where for example it would make 2 in a row, while not being able to avoid the opponent from winning two moves later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exercise 1.4: Learning from Exploration \n",
    "\n",
    "_Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "As a reminder, the temporal difference learning method may be denoted as such:\n",
    "\n",
    "$V(s) \\leftarrow V(s) + \\alpha\\bigg[V(s') - V(s)\\bigg]$\n",
    "\n",
    "Where $\\alpha$ denotes the step-size parameter, or learning rate, that is effectively reduced as the number of steps increases. The $\\epsilon$ value denotes the fraction or probability of randomly sampling an action as opposed to the greedy method of taking the action with the highest value.\n",
    "\n",
    "Not learning from exploratory **I think** means that $V(s)$ is not updated when an exploratory move is made. This happens with probability epsilon. \n",
    "\n",
    "** ----- WORK IN PROGRESS ----- **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exercise 1.5: Other Improvements \n",
    "\n",
    "_Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "_Modeling certain aspects of the game could help in finding optimal solutions. As mentioned model-based approaches are usually only useful when they are accurate enough. I do not foresee any difficulties in tic-tac-toe though, because it is quite simple. One could generate a tree of possible moves in search this tree for moves that are best to use._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-sutton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
